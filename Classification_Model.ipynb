{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.1.1-py3-none-win_amd64.whl (54.4 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\deepti\\anaconda3\\envs\\assignment\\lib\\site-packages (from xgboost) (1.18.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\deepti\\anaconda3\\envs\\assignment\\lib\\site-packages (from xgboost) (1.4.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier # \n",
    "import xgboost as xgb\n",
    "\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "\n",
    "# Used to ignore warnings generated from StackingCVClassifier\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets_52721_99691_student-mat.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>reason</th>\n",
       "      <th>guardian</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>schoolsup</th>\n",
       "      <th>famsup</th>\n",
       "      <th>paid</th>\n",
       "      <th>activities</th>\n",
       "      <th>nursery</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>GP</td>\n",
       "      <td>M</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>teacher</td>\n",
       "      <td>other</td>\n",
       "      <td>reputation</td>\n",
       "      <td>mother</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>GP</td>\n",
       "      <td>M</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>reputation</td>\n",
       "      <td>father</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>MS</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>services</td>\n",
       "      <td>teacher</td>\n",
       "      <td>home</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>teacher</td>\n",
       "      <td>teacher</td>\n",
       "      <td>course</td>\n",
       "      <td>mother</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    school sex  age address famsize Pstatus  Medu  Fedu      Mjob     Fjob  \\\n",
       "172     GP   M   17       U     LE3       T     4     4   teacher    other   \n",
       "119     GP   M   15       U     GT3       T     3     4     other    other   \n",
       "389     MS   F   18       U     GT3       T     1     1     other    other   \n",
       "198     GP   F   17       U     GT3       T     4     4  services  teacher   \n",
       "139     GP   F   15       U     GT3       T     4     4   teacher  teacher   \n",
       "\n",
       "         reason guardian  traveltime  studytime  failures schoolsup famsup  \\\n",
       "172  reputation   mother           1          2         0        no    yes   \n",
       "119  reputation   father           1          1         0        no     no   \n",
       "389      course   mother           2          2         1        no     no   \n",
       "198        home   mother           2          1         1        no    yes   \n",
       "139      course   mother           2          1         0        no     no   \n",
       "\n",
       "    paid activities nursery higher internet romantic  famrel  freetime  goout  \\\n",
       "172  yes        yes     yes    yes      yes       no       4         4      4   \n",
       "119   no         no     yes    yes      yes       no       3         4      3   \n",
       "389   no        yes     yes    yes       no       no       1         1      1   \n",
       "198   no         no     yes    yes      yes       no       4         2      4   \n",
       "139   no        yes     yes    yes      yes       no       4         3      2   \n",
       "\n",
       "     Dalc  Walc  health  absences  G1  G2  \n",
       "172     1     3       5         0  13  11  \n",
       "119     1     2       4         6  14  13  \n",
       "389     1     1       5         0   6   5  \n",
       "198     2     3       2        24  18  18  \n",
       "139     1     1       5         0  16  16  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop('G3', axis=1)\n",
    "y = df[['G3']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "num_transform = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "cat_transform = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('oneHot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = df.select_dtypes(include=['int64', 'float64']).drop(['G3'], axis=1).columns\n",
    "cat_features = df.select_dtypes(include=['object']).columns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transform, num_features),\n",
    "        ('cat', cat_transform, cat_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Support Vector classifier\n",
    "# c1 = SVC(C = 50, degree = 1, gamma = \"auto\", kernel = \"rbf\", probability = True)\n",
    "c1 = SVC(random_state = 1000)\n",
    "\n",
    "# Initializing Multi-layer perceptron  classifier\n",
    "# c2 = MLPClassifier(activation = \"relu\", alpha = 0.1, hidden_layer_sizes = (10,10,10),\n",
    "#                             learning_rate = \"constant\", max_iter = 2000, random_state = 1000)\n",
    "\n",
    "# Initialing Nu Support Vector classifier\n",
    "# classifier3 = NuSVC(degree = 1, kernel = \"rbf\", nu = 0.25, probability = True)\n",
    "\n",
    "# Initializing Random Forest classifier\n",
    "# c3 = RandomForestClassifier(n_estimators = 500, criterion = \"gini\", max_depth = 10,\n",
    "#                                      max_features = \"auto\", min_samples_leaf = 0.005,\n",
    "#                                      min_samples_split = 0.005, n_jobs = -1, random_state = 1000)\n",
    "\n",
    "\n",
    "c2 = xgb.XGBClassifier( objective = 'multi:softmax')\n",
    "\n",
    "c3 = RandomForestClassifier(random_state = 1000, n_jobs = -1)\n",
    "meta_c = SVC(probability = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_score': None,\n",
      " 'booster': None,\n",
      " 'colsample_bylevel': None,\n",
      " 'colsample_bynode': None,\n",
      " 'colsample_bytree': None,\n",
      " 'gamma': None,\n",
      " 'gpu_id': None,\n",
      " 'importance_type': 'gain',\n",
      " 'interaction_constraints': None,\n",
      " 'learning_rate': None,\n",
      " 'max_delta_step': None,\n",
      " 'max_depth': None,\n",
      " 'min_child_weight': None,\n",
      " 'missing': nan,\n",
      " 'monotone_constraints': None,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'num_parallel_tree': None,\n",
      " 'objective': 'multi:softmax',\n",
      " 'random_state': None,\n",
      " 'reg_alpha': None,\n",
      " 'reg_lambda': None,\n",
      " 'scale_pos_weight': None,\n",
      " 'subsample': None,\n",
      " 'tree_method': None,\n",
      " 'validate_parameters': None,\n",
      " 'verbosity': None}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(c2.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'randomforestclassifier__n_estimators': [20, 73, 126, 180, 233, 286, 340, 393, 446, 500], 'randomforestclassifier__max_features': ['auto', 'sqrt', 0.5], 'randomforestclassifier__max_depth': [5, 15, 26, 36, 47, 57, 68, 78, 89, 99, 110, None], 'randomforestclassifier__min_samples_split': [2, 5, 10], 'randomforestclassifier__min_samples_leaf': [1, 3, 5, 7], 'randomforestclassifier__bootstrap': [True, False], 'xgbclassifier__n_estimators': [10, 50, 100, 500], 'xgbclassifier__learning_rate': [0.1, 0.5, 1]}\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 500, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt',0.5]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 3, 5,7]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "params = {'randomforestclassifier__n_estimators': n_estimators,\n",
    "               'randomforestclassifier__max_features': max_features,\n",
    "               'randomforestclassifier__max_depth': max_depth,\n",
    "               'randomforestclassifier__min_samples_split': min_samples_split,\n",
    "               'randomforestclassifier__min_samples_leaf': min_samples_leaf,\n",
    "               'randomforestclassifier__bootstrap': bootstrap,\n",
    "              \"xgbclassifier__n_estimators\": [10, 50, 100, 500],\n",
    "                \"xgbclassifier__learning_rate\": [0.1, 0.5, 1],}\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifiers': [SVC(random_state=1000),\n",
      "                 XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              objective='multi:softmax', random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None),\n",
      "                 RandomForestClassifier(n_jobs=-1, random_state=1000)],\n",
      " 'cv': 2,\n",
      " 'drop_last_proba': False,\n",
      " 'meta_classifier': SVC(probability=True),\n",
      " 'meta_classifier__C': 1.0,\n",
      " 'meta_classifier__break_ties': False,\n",
      " 'meta_classifier__cache_size': 200,\n",
      " 'meta_classifier__class_weight': None,\n",
      " 'meta_classifier__coef0': 0.0,\n",
      " 'meta_classifier__decision_function_shape': 'ovr',\n",
      " 'meta_classifier__degree': 3,\n",
      " 'meta_classifier__gamma': 'scale',\n",
      " 'meta_classifier__kernel': 'rbf',\n",
      " 'meta_classifier__max_iter': -1,\n",
      " 'meta_classifier__probability': True,\n",
      " 'meta_classifier__random_state': None,\n",
      " 'meta_classifier__shrinking': True,\n",
      " 'meta_classifier__tol': 0.001,\n",
      " 'meta_classifier__verbose': False,\n",
      " 'n_jobs': None,\n",
      " 'pre_dispatch': '2*n_jobs',\n",
      " 'random_state': 42,\n",
      " 'randomforestclassifier': RandomForestClassifier(n_jobs=-1, random_state=1000),\n",
      " 'randomforestclassifier__bootstrap': True,\n",
      " 'randomforestclassifier__ccp_alpha': 0.0,\n",
      " 'randomforestclassifier__class_weight': None,\n",
      " 'randomforestclassifier__criterion': 'gini',\n",
      " 'randomforestclassifier__max_depth': None,\n",
      " 'randomforestclassifier__max_features': 'auto',\n",
      " 'randomforestclassifier__max_leaf_nodes': None,\n",
      " 'randomforestclassifier__max_samples': None,\n",
      " 'randomforestclassifier__min_impurity_decrease': 0.0,\n",
      " 'randomforestclassifier__min_impurity_split': None,\n",
      " 'randomforestclassifier__min_samples_leaf': 1,\n",
      " 'randomforestclassifier__min_samples_split': 2,\n",
      " 'randomforestclassifier__min_weight_fraction_leaf': 0.0,\n",
      " 'randomforestclassifier__n_estimators': 100,\n",
      " 'randomforestclassifier__n_jobs': -1,\n",
      " 'randomforestclassifier__oob_score': False,\n",
      " 'randomforestclassifier__random_state': 1000,\n",
      " 'randomforestclassifier__verbose': 0,\n",
      " 'randomforestclassifier__warm_start': False,\n",
      " 'shuffle': True,\n",
      " 'store_train_meta_features': False,\n",
      " 'stratify': True,\n",
      " 'svc': SVC(random_state=1000),\n",
      " 'svc__C': 1.0,\n",
      " 'svc__break_ties': False,\n",
      " 'svc__cache_size': 200,\n",
      " 'svc__class_weight': None,\n",
      " 'svc__coef0': 0.0,\n",
      " 'svc__decision_function_shape': 'ovr',\n",
      " 'svc__degree': 3,\n",
      " 'svc__gamma': 'scale',\n",
      " 'svc__kernel': 'rbf',\n",
      " 'svc__max_iter': -1,\n",
      " 'svc__probability': False,\n",
      " 'svc__random_state': 1000,\n",
      " 'svc__shrinking': True,\n",
      " 'svc__tol': 0.001,\n",
      " 'svc__verbose': False,\n",
      " 'use_clones': True,\n",
      " 'use_features_in_secondary': False,\n",
      " 'use_probas': False,\n",
      " 'verbose': 0,\n",
      " 'xgbclassifier': XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              objective='multi:softmax', random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=None, verbosity=None),\n",
      " 'xgbclassifier__base_score': None,\n",
      " 'xgbclassifier__booster': None,\n",
      " 'xgbclassifier__colsample_bylevel': None,\n",
      " 'xgbclassifier__colsample_bynode': None,\n",
      " 'xgbclassifier__colsample_bytree': None,\n",
      " 'xgbclassifier__gamma': None,\n",
      " 'xgbclassifier__gpu_id': None,\n",
      " 'xgbclassifier__importance_type': 'gain',\n",
      " 'xgbclassifier__interaction_constraints': None,\n",
      " 'xgbclassifier__learning_rate': None,\n",
      " 'xgbclassifier__max_delta_step': None,\n",
      " 'xgbclassifier__max_depth': None,\n",
      " 'xgbclassifier__min_child_weight': None,\n",
      " 'xgbclassifier__missing': nan,\n",
      " 'xgbclassifier__monotone_constraints': None,\n",
      " 'xgbclassifier__n_estimators': 100,\n",
      " 'xgbclassifier__n_jobs': None,\n",
      " 'xgbclassifier__num_parallel_tree': None,\n",
      " 'xgbclassifier__objective': 'multi:softmax',\n",
      " 'xgbclassifier__random_state': None,\n",
      " 'xgbclassifier__reg_alpha': None,\n",
      " 'xgbclassifier__reg_lambda': None,\n",
      " 'xgbclassifier__scale_pos_weight': None,\n",
      " 'xgbclassifier__subsample': None,\n",
      " 'xgbclassifier__tree_method': None,\n",
      " 'xgbclassifier__validate_parameters': None,\n",
      " 'xgbclassifier__verbosity': None}\n"
     ]
    }
   ],
   "source": [
    "pprint(stacking_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "# clf2 = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "# clf3 = GaussianNB()\n",
    "\n",
    "\n",
    "stacking_model = StackingCVClassifier(classifiers=[c1, c2, c3], \n",
    "                            meta_classifier=meta_c,\n",
    "                            random_state=42)\n",
    "\n",
    "# params = {'kneighborsclassifier__n_neighbors': [1, 5],\n",
    "#           'randomforestclassifier__n_estimators': [10, 50],\n",
    "#           'meta_classifier__C': [0.1, 10.0]}\n",
    "\n",
    "grid = GridSearchCV(estimator=stacking_model, \n",
    "                    param_grid=params, \n",
    "                    cv=3,\n",
    "                    refit=True)\n",
    "grid.fit(X, y)\n",
    "\n",
    "cv_keys = ('mean_test_score', 'std_test_score', 'params')\n",
    "\n",
    "for r, _ in enumerate(grid.cv_results_['mean_test_score']):\n",
    "    print(\"%0.3f +/- %0.2f %r\"\n",
    "          % (grid.cv_results_[cv_keys[0]][r],\n",
    "             grid.cv_results_[cv_keys[1]][r] / 2.0,\n",
    "             grid.cv_results_[cv_keys[2]][r]))\n",
    "\n",
    "print('Best parameters: %s' % grid.best_params_)\n",
    "print('Accuracy: %.2f' % grid.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
